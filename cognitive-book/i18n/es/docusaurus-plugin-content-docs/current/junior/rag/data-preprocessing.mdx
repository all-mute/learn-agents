---
sidebar_label: Preprocesamiento de datos
---

# Preprocesamiento de Datos

El preprocesamiento de datos es una etapa cr√≠tica en la construcci√≥n de sistemas RAG eficaces. La precisi√≥n y relevancia de las respuestas generadas por el modelo dependen directamente de la calidad de la preparaci√≥n de los datos. En esta secci√≥n, examinaremos los principales m√©todos y enfoques para preparar datos de texto para RAG.

*¬°El preprocesamiento de datos se puede aplicar no solo en RAG, sino que tambi√©n es √∫til en flujos de trabajo GenAI y agentes!*

:::tip
Los datos son el rey. La preparaci√≥n previa de los datos es el 50% del √©xito de RAG.

Si subestimas esta etapa, puedes perder el 50% de la relevancia de las respuestas. Basura entra, basura sale.
:::

## Preguntas

- Queremos dividir un documento en fragmentos para la vectorizaci√≥n. ¬øCu√°les son las formas de hacerlo?
- ¬øPor qu√© dividir en 1000 caracteres es una mala idea? ¬øY en 100 tokens?
- ¬øC√≥mo dividir documentos espec√≠ficos, por ejemplo, con html, json o c√≥digo?
- ¬øCu√°ndo podr√≠amos necesitar extraer texto de documentos aparentemente de texto?

## Pasos

### 1. Lea atentamente sobre [formas de dividir texto en fragmentos en langchain](https://python.langchain.com/docs/concepts/text_splitters/)

o mira la conferencia

<iframe width="560" height="315" src="https://www.youtube.com/embed/8OJC21T2SL4?si=IUYSuEf_9IwziIJA" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

### 2. Extracci√≥n de texto

Existe una gran cantidad de tipos de datos no estructurados: pdf, docx, rtf y similares. El problema con ellos es que pueden consistir en datos de diferentes formatos a la vez:
    - texto
    - tablas
    - im√°genes (incluidas infograf√≠as, dibujos, gr√°ficos)
    - etc.

Antes de que empecemos a dividir los documentos en fragmentos, debemos aprender a extraer texto de estos documentos. **La experiencia en producci√≥n muestra que es imposible prepararse globalmente para la extracci√≥n de texto: en cada caso hay escollos.** Sin embargo, hemos dejado aqu√≠ algunos materiales para ti, pero no es necesario estudiarlos.

1. [Upstage AI Document Parser: ¬°Revoluciona la extracci√≥n de datos PDF complejos!](https://youtu.be/Y2gXmCM3NMw?si=AhszbLJ23aUzdcJC)
2. [‚ú¶ Marker: Esta herramienta de c√≥digo abierto preparar√° tus PDF para LLM](https://youtu.be/Y2gXmCM3NMw?si=AhszbLJ23aUzdcJC)
3. [‚ú¶ Extracci√≥n de texto de archivos PDF para modelos de lenguaje grandes y RAG (PyMuPDF4llm üíö)](https://youtu.be/qjQNerPVrHg?si=EIibIR6mqoO1ZER3)

#### Bibliotecas de Python para la extracci√≥n de texto

- [pandas](https://pandas.pydata.org/) ‚Äì estructuras DataFrame/Series convenientes; indispensable para cargar, limpiar valores faltantes, combinar diferentes fuentes y preparar datos tabulares para la posterior generaci√≥n de ejemplos de texto.
- [scikit-learn](https://scikit-learn.org/) ‚Äì herramientas para escalar (StandardScaler, MinMaxScaler), codificar categor√≠as (OneHotEncoder, LabelEncoder) y dividir muestras (train_test_split); ejemplos: preparaci√≥n de caracter√≠sticas antes de entrenar un clasificador teniendo en cuenta el contexto generativo.
- [NLTK](https://www.nltk.org/) ‚Äì m√≥dulos cl√°sicos de NLP: tokenizaci√≥n, palabras vac√≠as, stemming/lematizaci√≥n; √∫til para el procesamiento inicial de textos antes de introducirlos en LLM.
- [spaCy](https://spacy.io/) ‚Äì an√°lisis r√°pido, NER, lematizaci√≥n y etiquetado POS; se utiliza para extraer entidades y estructurar el conocimiento en pipelines RAG.
- [regex](https://pypi.org/project/regex/) ‚Äì trabajo avanzado con expresiones regulares (soporte de Unicode, entornos POS); necesario para la limpieza de texto compleja y el an√°lisis de plantillas.
- [ftfy](https://ftfy.readthedocs.io/) ‚Äì "arregla el texto por ti": corrige Unicode roto, codificaciones HTML rotas y artefactos OCR; caso: limpieza de datos de diferentes web scrapings.
- [chardet](https://pypi.org/project/chardet/) ‚Äì detecci√≥n autom√°tica de la codificaci√≥n de archivos de texto; ayuda a leer correctamente documentos en diferentes codificaciones antes del preprocesamiento.
- [langdetect](https://pypi.org/project/langdetect/) ‚Äì biblioteca para la detecci√≥n del idioma del texto; se utiliza para soluciones RAG multiling√ºes, filtrado y enrutamiento de documentos por modelos de lenguaje.
- [clean-text](https://pypi.org/project/clean-text/) ‚Äì funciones listas para usar para limpiar texto: eliminar enlaces, emojis, caracteres especiales y espacios innecesarios; acelera la preparaci√≥n del corpus antes de la vectorizaci√≥n.
- [unstructured](https://github.com/Unstructured-IO/unstructured) ‚Äì reconocimiento y an√°lisis de PDF, DOCX, HTML, PPTX; extrae texto "limpio" y metadatos para crear fuentes de conocimiento.
- [Apache Tika](https://github.com/chrismattmann/tika-python) ‚Äì servicio para extraer texto/metadatos de muchos formatos; √∫til en pipelines ETL de grandes repositorios de documentos.
- [PDFPlumber](https://github.com/jsvine/pdfplumber) ‚Äì trabajo detallado con PDF: tablas, columnas, extracci√≥n de texto coordinada; adecuado para estructurar informes corporativos.
- [PyPDF2](https://pypi.org/project/PyPDF2/) ‚Äì funciones b√°sicas de lectura/escritura de PDF, combinaci√≥n y divisi√≥n de p√°ginas; se utiliza para preparar lotes de documentos para el almacenamiento vectorial.
- [BeautifulSoup4](https://www.crummy.com/software/BeautifulSoup/) ‚Äì an√°lisis de HTML/XML; se utiliza para recopilar y limpiar datos web antes de crear √≠ndices tipo wiki.

## Pasos Adicionales

### E1. C√≥mo establecer el tama√±o del fragmento en el divisor de documentos

<iframe width="560" height="315" src="https://www.youtube.com/embed/1bbDH3kyf9I?si=ITdTrQvmFabqffiO" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

### E2. Lectura adicional: [Dominar la divisi√≥n de texto para un RAG eficaz con Langchain](https://hidevscommunity.substack.com/p/mastering-text-splitting-for-effective)

## Ahora sabemos...

Hemos estudiado las t√©cnicas clave para extraer texto de varios formatos de datos y dividir el texto en fragmentos, necesarias para la preparaci√≥n de datos en RAG. La comprensi√≥n de estos m√©todos permite optimizar el proceso de indexaci√≥n y b√∫squeda de informaci√≥n relevante. Ahora est√° listo para aplicar este conocimiento en la pr√°ctica para mejorar sus aplicaciones RAG.

## Ejercicios

Preguntas para la reflexi√≥n, para que las neuronas del estudiante se muevan:

*   ¬øC√≥mo cambiar√° la calidad de las respuestas del sistema RAG si se utilizan tama√±os de fragmentos muy peque√±os o muy grandes? ¬øQu√© compromisos existen al elegir el tama√±o del fragmento?
*   Imagine que necesita procesar datos que contienen tablas y c√≥digo. ¬øQu√© estrategias de divisi√≥n de texto ser√≠an m√°s efectivas y por qu√© los divisores est√°ndar podr√≠an fallar?
*   En un proyecto real, los datos pueden provenir de diferentes fuentes (PDF, HTML, JSON, bases de datos) y tener diferentes estructuras. ¬øQu√© dificultades puede encontrar al crear un pipeline de preprocesamiento de datos universal y c√≥mo se pueden superar?
*   ¬øC√≥mo se puede evaluar la calidad de la divisi√≥n de texto en fragmentos antes de la etapa de construcci√≥n del √≠ndice y generaci√≥n de respuestas? ¬øExisten m√©tricas o enfoques para tal evaluaci√≥n?