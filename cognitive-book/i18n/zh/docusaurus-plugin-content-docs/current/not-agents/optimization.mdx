---
sidebar_label: 超参数优化
---

# AI 代理的超参数优化 ✦

让我们来学习微调 AI 系统的艺术：从手动调整提示词到创建自优化代理。我们将了解如何通过参数化的系统方法将“能用”变成“完美运行”。

自优化策略：自动改进提示词，调整代理/工作流的超参数，创建自适应对话场景。让机器人通过几代改进找到最佳解决方案——这是您创建自学习系统的秘诀！

## 问题

- 什么是超参数？
- LLM、工作流、代理有哪些超参数？
- 如何自动搜索最佳配置？
- 我们是否总能负担得起自动优化？

<details>
<summary>什么是参数和超参数？</summary>

*参数* - 通常是神经网络的权重 - 我们在训练过程中选择的数学量。

*超参数* - 是我们为优化模型性能而设置的参数。例如，对于神经网络，它可以是隐藏层中神经元的数量、特定的激活函数（有很多）等等。**对于 LLM**，它可以是温度、max_tokens、top_p 等。
</details>

## 超参数的种类

以下材料将讨论神经网络的超参数 - **但我请您在工作流和代理的上下文中考虑它们。** 参数可以是：
1. 整数 - 我们可以将值移动 +1 或 -100
2. 实数 - 我们可以将值移动任意量，例如 1e-10
3. 布尔值 - 我们可以打开/关闭特定模式
4. 字符串 - 我们可以更改文本
5. 列表 - 我们可以从值列表中选择

##### 对于代理，这可能是：
- 消息历史记录的令牌长度，达到该长度后我们将总结历史记录
- 温度
- 打开/关闭代理的特定模式
- 每个代理的提示词
- 代理的工具或子代理列表

##### 在基于 LLM 的服务的上下文中，超参数是我们为优化整个系统性能而设置的参数。例如：
- 对于**简单的聊天机器人**，这可能是：
  - 系统提示词
  - 温度
  - （这意味着我们将从几个提示词和温度中选择，以生成最高质量的答案）
- 对于 **RAG**，这可能是：
  - k（将用于增强答案的“上下文”块的数量）
  - chunk_size（块大小）
  - chunk_overlap（块重叠）
  - 文档、数据库的子区域
  - 各种嵌入模型
  - 文本预处理方法
  - 等等 + 更复杂的 RAG 具有更多的超参数
- 对于 **工作流**，这可能是：
  - 每个步骤的提示词
  - 结构化响应的结构类型
  - 用于路由的各种分类方法
  - 等等
- 对于 **代理**，这可能是：
  - 每个代理的提示词
  - max_iterations
  - 子代理
  - 架构
  - 工具
  - 工具描述
  - 从代理传递到代理的数据
  - 您想优化的任何内容

**示例：**
- 自动提示词调整：[一次性消灭提示词工程](https://youtu.be/OmTdkNEr2nU?si=Qx0M_PywhouQdHnb) 和 [两次性消灭提示词工程](https://youtu.be/Vn8A3BxfplE?si=RJLKyZA-7QhOMOPi)
- 系统的进化自发展
- 优化系统以实现最佳质量/价格/速度/效率等。

## 核心算法

文本材料：
[维基百科](https://en.wikipedia.org/wiki/Hyperparameter_optimization)
 & 
[插图](https://www.nb-data.com/p/6-common-hyperparameter-optimization)

视频：
- [超参数优化](https://youtu.be/ttE0F7fghfk?si=w9Cz3egxsw6aV1xv)
- [超参数优化小讲座](https://youtu.be/IqQT8se9ofQ?si=6YQBlsW5ry8Xzh_2)

![Ask ChatGPT](https://img.shields.io/badge/Ask%20ChatGPT-8A2BE2?style=for-the-badge)

#### 1. 网格搜索和随机搜索
    - 网格搜索：评估预定义集合中的所有组合。最适合小型、离散的空间，但可扩展性差。
    - 随机搜索：随机选择超参数，通常在高维空间中优于网格搜索，有效地探索更多值。

#### 2. 贝叶斯优化
    - 机制：使用概率模型（例如，高斯过程）来预测有希望的配置，从而平衡探索和利用。
    - 优点：对于低维任务有效，但在高维方面遇到困难。

#### 3. 进化算法
    - 过程：模拟自然选择，通过突变和交叉迭代地发展超参数集群体。
    - 应用：对于复杂的、不可区分的搜索空间（例如，神经架构搜索）有效。

#### 4. Hyperband 和 Successive Halving
    - Hyperband：将随机搜索与提前停止相结合，动态地将资源分配给有希望的配置。
    - Successive Halving：在训练的早期阶段积极地削减低效模型。

#### 5. 基于群体的训练 (PBT)
    - 自适应调整：在训练期间同时优化超参数和模型权重，非常适合动态任务，例如强化学习。

## 遗传算法

**视频资源：**
- [遗传算法与已解决的示例](https://www.youtube.com/watch?v=uQj5UNhCPuo)

## 手动优化

:::info
超参数优化通常分三个步骤完成：
1. 选择参数
2. 实验
3. 评估结果

    - 其中实验可以是大型问题/答案数据集或复杂的自动化环境，
    - 结果评估可以由人工评估员（通常这些评估员是您 :) ）或更昂贵的 LLM 进行

**如果此采样-实验-评估周期的总成本太高（例如，1000 美元），那么我们将无法构建用于自动优化的系统，而将手动执行此操作。**

然后，您可以从之前研究的算法中选择任何算法，并手动按照其说明进行操作。此外，在手动优化过程中，直觉和演绎推理将对您有很大帮助。
:::

## 现在我们知道了...

我们已经研究了什么是超参数，它们如何影响 GenAI 应用程序的性能，以及可以使用哪些优化方法来调整它们。现在，您可以应用这些知识来改进您的 AI 系统。

## 练习

回想一下您最近的一个项目：
- 您认为哪些超参数对您的项目最关键？
- 您会为您的任务选择哪些优化方法，为什么？

![Ask ChatGPT](https://img.shields.io/badge/Ask%20ChatGPT-8A2BE2?style=for-the-badge)