# LLM 工具使用基准测试

来自 https://www.perplexity.ai/page/understanding-llm-benchmarks-e-VZmXIq_FQgCIS.3QSVo6EA

随着 LLM 从纯粹的语言生成器过渡到能够与外部系统交互的活跃代理，工具使用基准测试代表了评估方法论中的一个关键前沿。

### ToolBench

ToolBench 专门评估 LLM 基于自然语言指令使用外部工具的能力。

**结构**：提供需要使用来自提供的 API 的工具的任务，评估模型选择适当工具并正确使用它们的能力。

**评估方法**：性能通过任务完成成功率和工具使用的正确性来衡量，包括工具选择、参数规范和结果解释。

**意义**：ToolBench 解决了 AI 代理中工具使用日益增长的重要性，这种能力将 LLM 扩展到纯粹的语言生成之外。

### API-Bank

API-Bank 评估 LLM 如何有效地与各种 API 交互以完成任务。

**结构**：API 规范和需要使用它们的任务的集合。

**评估方法**：衡量模型正确解释 API 文档、构建有效调用和适当处理响应的能力。

**意义**：对于评估 LLM 作为软件生态系统中的组件至关重要，在这些生态系统中，API 交互至关重要。

### ReAct 基准测试

ReAct 评估模型在基于环境的任务中交替进行推理和行动的能力。

**结构**：需要多步骤推理和工具使用来收集信息和完成目标的任务。

**评估方法**：评估推理过程（通过逐步思考展示）和行动准确性（正确的工具选择和使用）。

**意义**：与必须计划行动序列并根据中间结果进行调整的代理系统特别相关。

工具使用基准测试代表了超越纯粹语言生成评估的演变，解决了 LLM 作为复杂环境中活跃代理的角色日益增长的问题。随着模型在与外部系统交互的能力方面不断进步，这些基准测试对于全面评估将变得越来越重要。
