---
sidebar_label: Chatbots and Short-Term Memory
---

# Chatbots and Short-Term Memory

# Steps

### 1. Chatbots

[Explore the difference between chatbots and other things](https://excalidraw.com/#json=zjh38Ls_XMRTsUO60aur5,2ieU4682rDovB05fL_KgOw)

### 2. Studying short-term memory

What if a user interacts with your bot for the tenth time?
    - firstly, due to the large message history, the LLM follows the latest instructions worse
    - secondly, each subsequent request will cost more (if we use a proprietary LLM, all input-tokens are charged)

:::tip
just read both articles, look at the pictures - the main thing is to understand the concepts!

if you have carefully studied the OpenAI API, these articles will be simple and understandable for you.
:::

[How we handle message history](https://langchain-ai.github.io/langgraph/concepts/memory/#summarizing-past-conversations)

[Comparison of different approaches](https://www.pinecone.io/learn/series/langchain/langchain-expression-language/)

:::warning –º–∏–Ω—É—Å—ã
When deleting/summarizing messages, we lose useful information.

For example, the initial messages that we are going to delete may contain the user's specifications - and details will be lost when deleting/summarizing.
:::

#### Basic strategies for managing message history

1. **Token count trimming (`max_tokens`)**  
   Allows you to limit the message history so that their total number of tokens does not exceed a specified value. This is especially useful for complying with the model's context window limitations.

2. **Message count trimming (`max_messages`)**  
   Limits the history to a certain number of the latest messages, deleting older ones.

#### Additional parameters for trimming settings

- **`strategy="last"`**  
  Saves the latest messages, deleting older ones. This is the standard strategy for preserving relevant context.

- **`include_system=True`**  
  Ensures the `SystemMessage` is preserved, which usually contains important instructions for the model. ([How to trim messages | ü¶úÔ∏è LangChain](https://python.langchain.com/docs/how_to/trim_messages/?utm_source=chatgpt.com))

- **`start_on="human"` and `ends_on=("human", "tool")`**  
  Ensure the correct structure of the message history, starting with the user's message and ending with the user's or tool's message.

#### Usage examples

To trim the message history by the number of tokens while preserving the system message and the latest user messages, you can use the following code:

```python
from langchain_core.messages import trim_messages

trimmed_history = trim_messages(
    messages=chat_history,
    max_tokens=1000,
    strategy="last",
    include_system=True,
    start_on="human",
    ends_on=("human", "tool")
)
```

More information about the functions and parameters of `trim_messages` can be found in the official LangChain documentation:

- [How to trim messages](https://python.langchain.com/docs/how_to/trim_messages/)

#### Advanced techniques:

- we can use several short-term memories, for example:
    - one for storing message history - will be located in the messages themselves
    - the second for storing information about the user (for example, his preferences) - will be located in the system message
    - the third for storing the user's task - will be located in the system message
- we can do retrieve on deleted messages if necessary (retrieve we will study in the future [module](./rag/index))
