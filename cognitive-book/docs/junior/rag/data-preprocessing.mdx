---
sidebar_label: Data Preprocessing
---

# Data Preprocessing

Data Preprocessing is a critical step in building effective RAG systems. The accuracy and relevance of the answers generated by the model directly depend on the quality of data preparation. In this section, we will look at the main methods and approaches to preparing text data for RAG.

*You can apply data preprocessing not only in RAG - it is useful in GenAI Workflows and agents!*

:::tip
Data - is the king. Data preprocessing is 50% of the success for RAG.

If you underestimate this step, you may lose 50% of the relevance of the answers. Trash in, trash out.
:::

## Questions

- We want to cut the document into pieces for vectorization. What are the ways to do this?
- Why is cutting by 1000 characters a bad idea? What about 100 tokens?
- How to cut specific documents, such as html, json, or code?
- When might we need to extract text from seemingly text documents?

## Steps

### 1. Read carefully about [ways to split text into chunks in langchain](https://python.langchain.com/docs/concepts/text_splitters/)

or watch the lecture

<iframe width="560" height="315" src="https://www.youtube.com/embed/8OJC21T2SL4?si=IUYSuEf_9IwziIJA" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

### 2. Text extraction

There are a huge number of unstructured data types: pdf, docx, rtf and the like. The problem with them is that they can consist of data of different formats at once:
    - text
    - tables
    - pictures (including infographics, drawings, graphs)
    - etc.

Before we start breaking documents into chunks, we must learn how to extract text from these documents. **Experience in the field shows that it is impossible to globally prepare for text extraction: there are pitfalls in every case.** However, we have left some materials here for you - but studying them is not necessary.

1. [Upstage AI Document Parser: Revolutionise Complex PDF Data Extraction!](https://youtu.be/Y2gXmCM3NMw?si=AhszbLJ23aUzdcJC)
2. [âœ¦ Marker: This Open-Source Tool will make your PDFs LLM Ready](https://youtu.be/Y2gXmCM3NMw?si=AhszbLJ23aUzdcJC)
3. [âœ¦ Extracting Text from PDFs for Large Language Models and RAG (PyMuPDF4llm ðŸ’š)](https://youtu.be/qjQNerPVrHg?si=EIibIR6mqoO1ZER3)

#### Python libraries for text extraction

- [pandas](https://pandas.pydata.org/) â€“ convenient DataFrame/Series structures; indispensable for loading, cleaning gaps, combining different sources and preparing tabular data for subsequent generation of text examples.
- [scikit-learn](https://scikit-learn.org/) â€“ tools for scaling (StandardScaler, MinMaxScaler), encoding categories (OneHotEncoder, LabelEncoder) and splitting the sample (train_test_split); examples: preparing features before training a classifier taking into account the generative context.
- [NLTK](https://www.nltk.org/) â€“ classic NLP modules: tokenization, stop words, stemming/lemmatization; useful for initial text processing before feeding into the LLM.
- [spaCy](https://spacy.io/) â€“ fast parsing, NER, lemmatization and POS tagging; used to extract entities and structure knowledge in RAG pipelines.
- [regex](https://pypi.org/project/regex/) â€“ advanced work with regular expressions (Unicode support, POS environments); necessary for complex text cleaning and pattern analysis.
- [ftfy](https://ftfy.readthedocs.io/) â€“ Â«fixes text for youÂ»: fixing broken Unicode, broken HTML encodings and OCR artifacts; case: cleaning data from various web scrapings.
- [chardet](https://pypi.org/project/chardet/) â€“ automatic detection of text file encoding; helps to correctly read documents in different encodings before preprocessing.
- [langdetect](https://pypi.org/project/langdetect/) â€“ a library for determining the language of text; used for multilingual RAG solutions, filtering and routing documents by language models.
- [clean-text](https://pypi.org/project/clean-text/) â€“ ready-made text cleaning functions: removing links, emojis, special characters and extra spaces; speeds up corpus preparation before vectorization.
- [unstructured](https://github.com/Unstructured-IO/unstructured) â€“ recognition and parsing of PDF, DOCX, HTML, PPTX; extracts "clean" text and metadata to create knowledge sources.
- [ApacheÂ Tika](https://github.com/chrismattmann/tika-python) â€“ a service for extracting text/metadata from many formats; useful in ETL pipelines for large document repositories.
- [PDFPlumber](https://github.com/jsvine/pdfplumber) â€“ detailed work with PDF: tables, columns, coordinate text extraction; suitable for structuring corporate reports.
- [PyPDF2](https://pypi.org/project/PyPDF2/) â€“ basic PDF reading/writing functions, merging and splitting pages; used to prepare batches of documents for vector storage.
- [BeautifulSoup4](https://www.crummy.com/software/BeautifulSoup/) â€“ parsing HTML/XML; used to collect and clean web data before creating wiki-like indexes.

## Extra Steps

### E1. How to Set the Chunk Size in Document Splitter

<iframe width="560" height="315" src="https://www.youtube.com/embed/1bbDH3kyf9I?si=ITdTrQvmFabqffiO" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

### E2. Additional reading: [Mastering Text Splitting for Effective RAG with Langchain](https://hidevscommunity.substack.com/p/mastering-text-splitting-for-effective)

## Now we know...

We have studied the key techniques for extracting text from various data formats and splitting text into chunks, which are necessary for preparing data in RAG. Understanding these methods allows you to optimize the process of indexing and searching for relevant information. Now you are ready to apply this knowledge in practice to improve your RAG applications.

## Exercises

Food for thought to get the student's neurons moving:

*   How will the quality of RAG system responses change if you use very small or very large chunk sizes? What compromises exist when choosing a chunk size?
*   Imagine you need to process data containing tables and code. Which text splitting strategies will be most effective and why might standard splitters fail?
*   In a real project, data may come from different sources (PDF, HTML, JSON, databases) and have different structures. What difficulties might you encounter when creating a universal data preprocessing pipeline and how can they be overcome?
*   How can you assess the quality of text splitting into chunks before the index building and response generation stage? Are there any metrics or approaches for such an assessment?