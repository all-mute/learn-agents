# LLM Tool Use Benchmarks

from https://www.perplexity.ai/page/understanding-llm-benchmarks-e-VZmXIq_FQgCIS.3QSVo6EA

По мере того, как LLM переходят от чистых генераторов языка к активным агентам, способным взаимодействовать с внешними системами, тесты использования инструментов представляют собой критически важный рубеж в методологиях оценки.

### ToolBench

ToolBench конкретно оценивает способность LLM использовать внешние инструменты на основе инструкций на естественном языке.

**Структура**: Представляет задачи, требующие использования инструментов из предоставленного API, оценивая способность модели выбирать подходящие инструменты и использовать их правильно.

**Метод оценки**: Производительность измеряется по показателю успешности выполнения задачи и правильности использования инструмента, включая выбор инструмента, спецификацию параметров и интерпретацию результатов.

**Значимость**: ToolBench решает растущую важность использования инструментов в AI Agents, возможности, которая расширяет LLM за пределы чистой генерации языка.

### API-Bank

API-Bank оценивает, насколько эффективно LLM могут взаимодействовать с различными API для выполнения задач.

**Структура**: Коллекция спецификаций API и задач, требующих их использования.

**Метод оценки**: Измеряет способность модели правильно интерпретировать документацию API, создавать допустимые вызовы и надлежащим образом обрабатывать ответы.

**Значимость**: Критически важен для оценки LLM как компонентов в программных экосистемах, где взаимодействие с API имеет важное значение.

### ReAct Benchmark

ReAct оценивает способность модели чередовать рассуждения и действия в задачах, основанных на окружении.

**Структура**: Задачи, требующие многошаговых рассуждений и использования инструментов для сбора информации и достижения целей.

**Метод оценки**: Оценивает как процесс рассуждения (продемонстрированный посредством поэтапного мышления), так и точность действий (правильный выбор и использование инструмента).

**Значимость**: Особенно актуально для agent систем, которые должны планировать последовательности действий и адаптироваться на основе промежуточных результатов.

Tool use benchmarks представляют собой эволюцию за пределы оценки чистой генерации языка, учитывая растущую роль LLM как активных агентов в сложных средах. Поскольку модели продолжают совершенствоваться в своей способности взаимодействовать с внешними системами, эти тесты станут все более важными для всесторонней оценки.
