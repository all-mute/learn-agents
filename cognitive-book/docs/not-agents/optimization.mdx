---
title: Оптимизация гиперпараметров
---

### Optimization Algorithms ✦✦✦

#### Виды гиперпараметров

Стратегии для самооптимизации: автоматически улучшаем промпты, настраиваем гиперпараметры агентов/workflow, создаем адаптивные диалоговые сценарии. Позволяет ботам находить оптимальные решения через поколения улучшений — ваш секрет создания самообучающихся систем!

<details>
<summary>Что такое параметры и гиперпараметры?</summary>
*Параметры* - обычно это веса нейронной сети - математические величины, которые мы подбираем в процессе обучения.

*Гиперпараметры* - это параметры, которые мы настраиваем для оптимизации работы модели. Например,для нейронной сети это может быть количество нейронов в скрытом слое, определенная функция активации (их существует много) и т.д. **Для LLM** это может быть температура, max_tokens, top_p и т.д.
</details>

В материалах ниже речь будет идти о гиперпараметрах для нейронных сетей - **но я прошу вас думать о них в контексте workflows и агентов.** Параметры бывают:
1. целочисленные - можем двигать значение на +1 или -100
2. вещественные - можем двигать значение на любую величину, например на 1e-10
3. булевые - можем включать/выключать определенный режим
4. строковые - можем менять текст
5. списковые - можем выбирать из списка значений

Для агетнов это может быть:
- длинна истории сообщений в токенах, после достижения которой мы суммаризируем историю
- температура
- включение/выключение определенных режимов вашего агента
- промпты для каждого агента
- список инструментов или под-агентов для агента

В контексте LLM-based сервисов гиперпараметры - это параметры, которые мы настраиваем для оптимизации работы всей системы. Например: 
- для простого чат-бота это может быть:
  - системный промпт
  - температура
  - (значит, мы будем выбирать из нескольких промптов и температур, для генерации наиболее качественных ответов)
- для RAG это могут быть:
  - k (количество чанков "контекста", которые будут использоваться для аугментации ответа)
  - chunk_size (размер чанка)
  - chunk_overlap (перекрытие чанков)
  - подобласти документов, баз данных
  - различные эмбеддинг-модели
  - методы предобработки текста
  - и т.д. + у более сложных RAG еще больше гиперпараметров
- для workflow это могут быть:
  - промпты для каждого шага
  - типы структур для структурных ответов
  - различные способы классификаций для routing
  - и т.д.
- для агентов это могут быть:
  - промпты для каждого агента
  - max_iterations
  - подагенты
  - архитектура
  - инструменты
  - описания инструментов
  - передающиеся от агента к агенту данные
  - всё что вы только захотите оптимизировать

**Примеры:**
- Автоматическая настройка промптов: [убиваем промпт-инжиниринг раз](https://youtu.be/OmTdkNEr2nU?si=Qx0M_PywhouQdHnb) и [два](https://youtu.be/Vn8A3BxfplE?si=RJLKyZA-7QhOMOPi)
- Эволюционное саморазвитие системы
- Оптимизация системы для достижения наилучшего качества/цены/скорости/эффективности и т.д.


#### Core Algorithms

Text materials:
[Wikipedia](https://en.wikipedia.org/wiki/Hyperparameter_optimization)
[Иллюстрации](https://www.nb-data.com/p/6-common-hyperparameter-optimization)

Videos:
- [Hyperparameter Optimization](https://youtu.be/ttE0F7fghfk?si=w9Cz3egxsw6aV1xv)
- [Hyperparameter Optimization small lecture](https://youtu.be/IqQT8se9ofQ?si=6YQBlsW5ry8Xzh_2)

1. Grid Search & Random Search
- Grid Search: Evaluates all combinations in a predefined set. Best for small, discrete spaces but scales poorly.
- Random Search: Randomly samples hyperparameters, often outperforming grid search in high-dimensional spaces by exploring more values efficiently.

2. Bayesian Optimization
Mechanism: Uses probabilistic models (e.g., Gaussian Processes) to predict promising configurations, balancing exploration and exploitation.

Strengths: Efficient for low-dimensional problems but struggles with high dimensionality.

3. Evolutionary Algorithms
Process: Mimics natural selection by iteratively evolving populations of hyperparameter sets via mutation and crossover.

Use Case: Effective for complex, non-differentiable search spaces (e.g., neural architecture search).

4. Hyperband & Successive Halving
Hyperband: Combines random search with early stopping, dynamically allocating resources to promising configurations.

Successive Halving: Aggressively prunes underperforming models early in training.

5. Population-Based Training (PBT)
Adaptive Tuning: Simultaneously optimizes hyperparameters and model weights during training, ideal for dynamic tasks like reinforcement learning.

#### Genetic Algorithms

**Видео ресурсы:**
- [Genetic Algorithm with Solved Example](https://www.youtube.com/watch?v=uQj5UNhCPuo)
- [Genetic Algorithms Explained By Example](https://www.youtube.com/watch?v=uCXm6avugCo)
- [Introduction to Genetic Algorithms — Including Example Code](https://www.youtube.com/watch?v=9zfeTw-uFCw)

**LeetCode задачи:**
- [Traveling Salesman Problem](https://leetcode.com/discuss/interview-question/2449410/google-oa-traveling-salesman-problem)
- [N-Queens](https://leetcode.com/problems/n-queens/)[5]
- [Maximize Score After N Operations](https://leetcode.com/problems/maximize-score-after-n-operations/)

#### Ручная Оптимизация

:::info
Оптимизация гиперпараметров зачастую выполняется в три шага:
1. Выбор параметров
2. Эксперимент
3. Оценка результатов

- где эксперимент может быть большим датасетом вопросов/ответов или сложной автоматической средой,
- а оценка результатов может проводиться людьми-ассесорами (чаще всего эти ассессоры - вы :) ) или более дорогими LLM

**Если общая стоимость данного цикла семплинг-эксперимент-оценка слишком высокая (например, 1000$), то мы не сможем строить систему для автоматической оптимизации и будем делать это вручную.**

Тогда вы можете выбрать любой алгоритм из изученных ранее, и использовать его для ручной оптимизации. Также при ручной оптимизации вам сильно помогут интуиция и дедуктивные рассуждения.
:::



